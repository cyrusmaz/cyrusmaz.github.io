{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhR_I5srJubt"
   },
   "outputs": [],
   "source": [
    "# Partial Bayesian Neural Networks\n",
    "# Cyrus Maz & Nathan Friedman 2019\n",
    "# Builds on code written by David Duvenaud\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.scipy.stats.norm as norm\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "\n",
    "def shapes_and_num(layer_sizes):\n",
    "    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
    "    N_weights = sum((m + 1) * n for m, n in shapes)\n",
    "    return shapes, N_weights\n",
    "\n",
    "\n",
    "def unpack_layers(weights, layer_sizes):\n",
    "    \"\"\" unpacks weights [ns, nw] into each layers relevant tensor shape\"\"\"\n",
    "    shapes, _ = shapes_and_num(layer_sizes)\n",
    "    n_samples = len(weights)\n",
    "    for m, n in shapes:\n",
    "        yield weights[:, :m * n].reshape((n_samples, m, n)), \\\n",
    "              weights[:, m * n:m * n + n].reshape((n_samples, 1, n))\n",
    "        weights = weights[:, (m + 1) * n:]\n",
    "\n",
    "\n",
    "def reshape_weights(weights, layer_sizes):\n",
    "    return list(unpack_layers(weights, layer_sizes))\n",
    "\n",
    "\n",
    "def bnn_predict(weights, inputs, layer_sizes, activiation_fcn):\n",
    "    if len(inputs.shape)<3: inputs = np.expand_dims(inputs, 0)  # [1,N,D]\n",
    "    weights = reshape_weights(weights, layer_sizes)\n",
    "    for W, b in weights:\n",
    "        #print(W.shape, inputs.shape)\n",
    "        outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n",
    "        inputs = activiation_fcn(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def diag_gaussian_log_density(x, mu, log_std):\n",
    "    return np.sum(norm.logpdf(x, mu, np.exp(log_std)), axis=0)  # [ns]\n",
    "\n",
    "def logqu(x, mu, std):\n",
    "    x = -np.sum(np.log(std)) - np.matmul(np.matmul(x-mu,np.diag(np.reciprocal(std**2))),x-mu)\n",
    "    \n",
    "    return -(len(mu)/2.)*np.log(2*np.pi) + x\n",
    "\n",
    "\n",
    "def sample_weights(params, N_samples):\n",
    "    mean, log_std = params\n",
    "    \n",
    "    exp_vector = np.array([0 if np.isnan(x) == True else np.exp(x) for x in log_std])\n",
    "    \n",
    "    output=rs.randn(N_samples, mean.shape[0]) * exp_vector + mean  # [ns, nw]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def sample_bnn(params, x, N_samples, layer_sizes, act):\n",
    "    bnn_weights = sample_weights(params, N_samples)\n",
    "    \n",
    "    f_bnn = bnn_predict(bnn_weights, x, layer_sizes, act)[:, :, 0]\n",
    "    return f_bnn # [ns, nd]\n",
    "\n",
    "\n",
    "def log_pdf_prior(weights, prior_params, sd):\n",
    "    if prior_params is None:\n",
    "        return diag_gaussian_log_density(weights, 0, np.log(sd))\n",
    "    else:\n",
    "        prior_mean, prior_log_std = prior_params\n",
    "        return diag_gaussian_log_density(weights, prior_mean, prior_log_std)\n",
    "\n",
    "def log_pdf_prior2(weights, prior_params, sd):\n",
    "    if prior_params == None:\n",
    "        d = len(weights)\n",
    "        x = -(d/2)*np.log(2*np.pi)- d*np.log(sd)- np.sum(weights**2)/(2*sd^2)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def log_like(y, f, sd):\n",
    "    d = np.shape(f)[0]*np.shape(f)[1]\n",
    "    a = d*np.log(sd)\n",
    "    c = (d/2.)*np.log(2*np.pi)\n",
    "    SS = np.sum((f - y)**2, axis=1) / (2*sd**2)\n",
    "    return - c - a - SS  # [ns]\n",
    "\n",
    "\n",
    "\n",
    "def vlb_objective(params, x, y, layer_sizes, n_samples, \n",
    "                  prior_sd=10, model_sd=0.1, prior_params=None, act=np.tanh):\n",
    "    Pweight = np.argwhere(np.isnan(params[1]) == False)\n",
    "    Fweight = np.argwhere(np.isnan(params[1]) == True)\n",
    "    mean, log_std = params\n",
    "    log_std = log_std[Pweight]\n",
    "    \n",
    "    \n",
    "    \n",
    "    weights = sample_weights(params, n_samples)\n",
    "    \n",
    "    if (len(Pweight)==0): \n",
    "      logq=[0]\n",
    "      log_prior=0\n",
    "    else: \n",
    "      logq = np.array([logqu(weights[k,Pweight], mu = params[0][Pweight], std = np.exp(params[1][Pweight])) for k in range(n_samples)])\n",
    "      log_prior = np.array([log_pdf_prior2(weights[k,Pweight], prior_params, prior_sd) for k in range(n_samples)])    \n",
    "  \n",
    "    f_bnn = bnn_predict(weights, x, layer_sizes, act)[:, :, 0]\n",
    "   \n",
    "    log_likelihood = log_like(y.T, f_bnn, model_sd)\n",
    "   \n",
    "    retval = np.mean(logq[0] -log_likelihood - log_prior)\n",
    "    \n",
    "    return retval\n",
    "\n",
    "\n",
    "\n",
    "def init_var_params(layer_sizes, Pweights,scale=-5, scale_mean=1):\n",
    "    _, num_weights = shapes_and_num(layer_sizes)\n",
    "    X = rs.randn(num_weights)*scale_mean, np.ones(num_weights)*scale\n",
    "    X[1][Pweights] = np.nan\n",
    "    \n",
    "    return  X # mean, log_std\n",
    "\n",
    "## Build data sets:\n",
    "def build_dataset_1(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "\n",
    "    targets = inputs + rs.randn(n_data) * noise_std\n",
    "    inputs = inputs\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "\n",
    "def build_dataset_2(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "\n",
    "    targets = inputs**2 /10 + rs.randn(n_data) * noise_std\n",
    "    inputs = inputs\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "def build_dataset_3(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "\n",
    "    targets = 3*np.sin(inputs)+inputs + rs.randn(n_data) * noise_std\n",
    "    inputs = inputs\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "\n",
    "def build_dataset_4(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "\n",
    "    targets = np.log(inputs**2 )*3 + rs.randn(n_data) * noise_std\n",
    "    inputs = inputs\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "def build_dataset_5(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "\n",
    "    targets = 6*np.sin(inputs**2 /15)**2+ 6*np.cos(inputs**2 /15)+rs.randn(n_data) * noise_std\n",
    "    inputs = inputs\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "def build_dataset_6(n_data=50, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    # inputs = np.linspace(-n_data/2+10, n_data/2+10, num=n_data)\n",
    "    inputs = np.linspace(-15, 15, num=n_data)\n",
    "    targets = (inputs**2)/10 * np.sin(inputs**2 / 20 + inputs) + inputs/20 + np.cos(inputs)+ rs.randn(n_data) * noise_std\n",
    "\n",
    "\n",
    "    return inputs[:,None], targets[:,None]\n",
    "\n",
    "\n",
    "def plot_mean_std(x_plot, p, D, path):\n",
    "    x, y = D\n",
    "    # col = colors[plot]\n",
    "    col = sns.xkcd_rgb[\"watermelon\"]\n",
    "\n",
    "    mean, std = np.mean(p, axis=1), np.std(p, axis=1)\n",
    "\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x_plot, mean, lw=2)\n",
    "    ax.fill_between(x_plot, mean - 1.96 * std, mean + 1.96 * std, color=col)  # 95% CI\n",
    "    ax.plot(x, y, 'ko', ms=4)\n",
    "    # ax.tick_params(labelleft='off', labelbottom='off')\n",
    "    ax.set_ylim([y_lower, y_upper])\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    plt.savefig(path, bbox_inches='tight',dpi = 300)\n",
    "    plt.close() \n",
    "\n",
    "def plot_samples(x_plot, p, D, path):\n",
    "    x, y = D\n",
    "\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x_plot, p, lw=1)\n",
    "    ax.plot(x, y, 'ko', ms=4)\n",
    "    # ax.tick_params(labelleft='off', labelbottom='off')\n",
    "    ax.set_ylim([y_lower, y_upper])\n",
    "    ax.set_xlim([x_lower, x_upper])\n",
    "    \n",
    "    plt.savefig(path, bbox_inches='tight',dpi = 300)\n",
    "    plt.close()     \n",
    "\n",
    "\n",
    "    \n",
    "rbf = lambda x: np.exp(-x ** 2)\n",
    "\n",
    "def objective(params,t):\n",
    "    return vlb_objective(params, inputs, targets, arch, n_samples=10, act=rbf)\n",
    "        \n",
    "def callback_func(params,t,g):\n",
    "    Obj_count[t] = -objective(params, t)\n",
    "    if t % 200 == 0:\n",
    "        print(\"ITER {} | OBJ {}\".format(t, -objective(params, t)))        \n",
    "\n",
    "def RandomSam(arch,k):\n",
    "    Tweights = sum([arch[x]*arch[x+1] + arch[x+1] for x in range(len(arch)-1)])\n",
    "    return(rs.choice(range(Tweights),k, replace = False))\n",
    "\n",
    "def functions(x,numfun):\n",
    "    if numfun == 1:\n",
    "        y = x\n",
    "    elif numfun == 2:\n",
    "        y = x**2 /10\n",
    "    elif numfun == 3:\n",
    "        y = 3*np.sin(x) + x\n",
    "    elif numfun == 4:\n",
    "        y = 3*np.log(x**2)\n",
    "    elif numfun == 5:\n",
    "        y = 6*np.sin(x**2 /15)**2+ 6*np.cos(x**2 /15)\n",
    "    elif numfun == 6:\n",
    "        y = (1./10.)* x**2 * np.sin(x**2 /20 + x) + x/20 + np.cos(x)\n",
    "    return(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "tiarEmVYnl4s",
    "outputId": "beb618a6-bc25-41bc-e13d-fb77677ae002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# authorize google colab to access Google Drive for saving plots\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_yBPVKuZaus"
   },
   "outputs": [],
   "source": [
    "file_directory='gdrive/My Drive/CSC2516_project/plots_15/'\n",
    "\n",
    "# Training data parameters\n",
    "noise_std=0.5\n",
    "n_data=40\n",
    "\n",
    "# Adam Parameters\n",
    "step_size=0.005\n",
    "num_iters=601\n",
    "\n",
    "# NN Parameters\n",
    "N_samples = 10\n",
    "arch = [1, 40, 40, 1]\n",
    "\n",
    "# Plot Parameters\n",
    "y_lower=-30\n",
    "y_upper=30\n",
    "x_lower=-30\n",
    "x_upper=30\n",
    "num_xs = 600\n",
    "\n",
    "# Training Parameters\n",
    "numper = [1,10,6,3,1]\n",
    "kvals = [0,180,440,880,1761]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2934
    },
    "colab_type": "code",
    "id": "GgU2gYV5JucD",
    "outputId": "073cd360-3f7e-4e77-a9ee-e7459a9b01b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Ensemble of 1 networks;\n",
      "*** 0.0% bayesian;\n",
      "*** function 5\n",
      "*** network 1 of 1;\n",
      "ITER 0 | OBJ -64884.51401482076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/autograd/numpy/numpy_vjps.py:444: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return lambda g: g[idxs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER 200 | OBJ -14467.83868628395\n",
      "ITER 400 | OBJ -9582.660878687404\n",
      "ITER 600 | OBJ -4406.729823895868\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_0_nn_1_of_1.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k0_nn_1_of_1.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k0_nn_1_of_1.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 1 of 10;\n",
      "ITER 0 | OBJ -87670.73632442483\n",
      "ITER 200 | OBJ -14640.608011833325\n",
      "ITER 400 | OBJ -10706.746849212246\n",
      "ITER 600 | OBJ -7259.2502020788215\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_1_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_1_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_1_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 2 of 10;\n",
      "ITER 0 | OBJ -248735.6441895409\n",
      "ITER 200 | OBJ -14475.615099305889\n",
      "ITER 400 | OBJ -11058.662638041424\n",
      "ITER 600 | OBJ -9369.911534508601\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_2_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_2_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_2_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 3 of 10;\n",
      "ITER 0 | OBJ -57446.945036909936\n",
      "ITER 200 | OBJ -13641.319140759904\n",
      "ITER 400 | OBJ -7802.854384228699\n",
      "ITER 600 | OBJ -4500.606873646085\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_3_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_3_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_3_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 4 of 10;\n",
      "ITER 0 | OBJ -79414.90580268703\n",
      "ITER 200 | OBJ -14233.864487596726\n",
      "ITER 400 | OBJ -10020.389911805296\n",
      "ITER 600 | OBJ -8403.718748705138\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_4_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_4_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_4_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 5 of 10;\n",
      "ITER 0 | OBJ -50746.14746109556\n",
      "ITER 200 | OBJ -15610.12649400164\n",
      "ITER 400 | OBJ -14280.564935181523\n",
      "ITER 600 | OBJ -10395.48575844012\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_5_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_5_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_5_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 6 of 10;\n",
      "ITER 0 | OBJ -111640.07097558957\n",
      "ITER 200 | OBJ -17042.918727051005\n",
      "ITER 400 | OBJ -13550.817709599698\n",
      "ITER 600 | OBJ -10833.543077542654\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_6_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_6_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_6_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 7 of 10;\n",
      "ITER 0 | OBJ -128372.0132022127\n",
      "ITER 200 | OBJ -18140.989920942546\n",
      "ITER 400 | OBJ -14982.538250911908\n",
      "ITER 600 | OBJ -9393.540706306585\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_7_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_7_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_7_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 8 of 10;\n",
      "ITER 0 | OBJ -68755.88093895247\n",
      "ITER 200 | OBJ -16773.371615194097\n",
      "ITER 400 | OBJ -10202.306529531212\n",
      "ITER 600 | OBJ -4189.378464530306\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_8_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_8_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_8_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 9 of 10;\n",
      "ITER 0 | OBJ -146712.42161269602\n",
      "ITER 200 | OBJ -15388.360518358799\n",
      "ITER 400 | OBJ -10013.790497874867\n",
      "ITER 600 | OBJ -6530.458362199449\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_9_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_9_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_9_of_10.png\n",
      "*** Ensemble of 10 networks;\n",
      "*** 10.0% bayesian;\n",
      "*** function 5\n",
      "*** network 10 of 10;\n",
      "ITER 0 | OBJ -215722.55833377977\n",
      "ITER 200 | OBJ -16612.833603379604\n",
      "ITER 400 | OBJ -14035.145548905211\n",
      "ITER 600 | OBJ -12073.65740421484\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_180_nn_10_of_10.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k180_nn_10_of_10.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k180_nn_10_of_10.png\n",
      "*** Ensemble of 6 networks;\n",
      "*** 25.0% bayesian;\n",
      "*** function 5\n",
      "*** network 1 of 6;\n",
      "ITER 0 | OBJ -55187.9156095404\n",
      "ITER 200 | OBJ -14312.597370105043\n",
      "ITER 400 | OBJ -6091.2212834551265\n",
      "ITER 600 | OBJ -4166.695678331872\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_440_nn_1_of_6.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k440_nn_1_of_6.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k440_nn_1_of_6.png\n",
      "*** Ensemble of 6 networks;\n",
      "*** 25.0% bayesian;\n",
      "*** function 5\n",
      "*** network 2 of 6;\n",
      "ITER 0 | OBJ -48110.97168259329\n",
      "ITER 200 | OBJ -18318.81270530537\n",
      "ITER 400 | OBJ -13314.606718058558\n",
      "ITER 600 | OBJ -9975.22677741565\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_440_nn_2_of_6.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k440_nn_2_of_6.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k440_nn_2_of_6.png\n",
      "*** Ensemble of 6 networks;\n",
      "*** 25.0% bayesian;\n",
      "*** function 5\n",
      "*** network 3 of 6;\n",
      "ITER 0 | OBJ -226383.00876538953\n",
      "ITER 200 | OBJ -22069.21325886943\n",
      "ITER 400 | OBJ -18550.15708703148\n",
      "ITER 600 | OBJ -15196.39919819452\n",
      "Learning Curve plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_learn_curve_k_440_nn_3_of_6.png\n",
      "CI plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_CI_k440_nn_3_of_6.png\n",
      "Sample predictions plot done: gdrive/My Drive/CSC2516_project/plots_15/f5_samples_k440_nn_3_of_6.png\n",
      "*** Ensemble of 6 networks;\n",
      "*** 25.0% bayesian;\n",
      "*** function 5\n",
      "*** network 4 of 6;\n",
      "ITER 0 | OBJ -55728.25099850823\n",
      "ITER 200 | OBJ -16925.323126501647\n",
      "ITER 400 | OBJ -8861.16587362163\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(file_directory): os.makedirs(file_directory);\n",
    "\n",
    "plot_inputs = np.linspace(x_lower, x_upper, num=num_xs)  \n",
    "\n",
    "# set seed for reproducability   \n",
    "rs = npr.RandomState(2516)\n",
    "\n",
    "# Loop:\n",
    "for n in range(1,6):\n",
    "\n",
    "    if n==1: inputs, targets = build_dataset_1(n_data,noise_std);\n",
    "    if n==2: inputs, targets = build_dataset_2(n_data,noise_std);\n",
    "    if n==3: inputs, targets = build_dataset_3(n_data,noise_std);\n",
    "    if n==4: inputs, targets = build_dataset_4(n_data,noise_std);\n",
    "    if n==5: inputs, targets = build_dataset_5(n_data,noise_std); \n",
    "    if n==6: inputs, targets = build_dataset_6(n_data,noise_std); num_iters=1001\n",
    "\n",
    "    Preds = np.zeros((len(kvals),num_xs))\n",
    "\n",
    "    for l in range(len(kvals)):\n",
    "\n",
    "        Temp_hold = np.zeros((numper[l],num_xs))\n",
    "\n",
    "        for j in range(numper[l]):\n",
    "          \n",
    "            print(\"*** Ensemble of \" + str(numper[l]) + \" networks;\")    \n",
    "            print(\"*** \" + str(round(kvals[l]/1761 ,2)*100) + \"% bayesian;\")\n",
    "            print(\"*** function \" +str(n))\n",
    "            print(\"*** network \" + str(j+1) + \" of \" + str(numper[l]) + \";\")\n",
    "            \n",
    "            k = kvals[l]\n",
    "            r  = 1761 - k\n",
    "            Pweights = RandomSam(arch,r)\n",
    "\n",
    "            init_var_param = init_var_params(arch,Pweights)\n",
    "\n",
    "\n",
    "            Obj_count = np.zeros(num_iters)\n",
    "            var_params = adam(grad(objective), init_var_param,\n",
    "                      step_size=step_size, num_iters=num_iters, callback=callback_func)\n",
    "\n",
    "            # Plot learning curve:\n",
    "            learning_curve_plot_name = \"f\"+str(n)+\"_learn_curve_k_\"+str(kvals[l])+\"_nn_\"+str(j+1)+\"_of_\"+str(numper[l])+\".png\"\n",
    "            learning_curve_plot_path = file_directory + learning_curve_plot_name\n",
    "            plt.plot(Obj_count)\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            plt.ylabel(\"Objective\")\n",
    "            plt.savefig(learning_curve_plot_path, bbox_inches='tight',dpi = 300)\n",
    "            plt.close() \n",
    "            \n",
    "            print(\"Learning Curve plot done: \"+learning_curve_plot_path)\n",
    "\n",
    "            f_bnn = sample_bnn(var_params, plot_inputs[:, None], N_samples, arch, rbf)\n",
    "            yhat = np.mean(f_bnn,axis = 0) \n",
    "            Temp_hold[j,] = yhat\n",
    "            \n",
    "            \n",
    "            # Plot CI's\n",
    "            CI_plot_name= \"f\"+str(n)+\"_CI_k\"+str(kvals[l])+\"_nn_\"+str(j+1)+\"_of_\"+str(numper[l])+\".png\"\n",
    "            CI_plot_path= file_directory+CI_plot_name\n",
    "            \n",
    "            samples_plot_name= \"f\"+str(n)+\"_samples_k\"+str(kvals[l])+\"_nn_\"+str(j+1)+\"_of_\"+str(numper[l])+\".png\"\n",
    "            samples_plot_path= file_directory+samples_plot_name\n",
    "            \n",
    "#             sns.set_style(\"white\")\n",
    "            \n",
    "            D=inputs,targets\n",
    "            \n",
    "            f_bnn=np.transpose(f_bnn)\n",
    "            plot_mean_std(plot_inputs, f_bnn, D, path=CI_plot_path) ####\n",
    "            print(\"CI plot done: \"+ CI_plot_path)\n",
    "            \n",
    "            plot_samples(plot_inputs, f_bnn, D, path=samples_plot_path) ####\n",
    "            print(\"Sample predictions plot done: \"+samples_plot_path)            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        Preds[l,] = np.mean(Temp_hold,axis = 0)\n",
    "\n",
    "\n",
    "    # Plot Regressions \n",
    "    regression_plot_name=\"function\"+str(n)+\".png\"\n",
    "    regression_plot_path=file_directory+regression_plot_name\n",
    "\n",
    "    y_act = functions(plot_inputs,n) \n",
    "    To_plot = np.c_[y_act,np.transpose(Preds)]\n",
    "\n",
    "    plot_labs = ['Observations','y(x)','Non-Bayesian', '10% Bayesian', '25% Bayesian','50% Bayesian' ,'Full Bayesian']\n",
    "    plt.plot(inputs,targets, \"o\", alpha = 0.2, color = 'black')\n",
    "    plt.plot(plot_inputs,To_plot)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend(plot_labs,loc='best')\n",
    "    plt.savefig(regression_plot_path, bbox_inches='tight',dpi = 300)\n",
    "    plt.close() \n",
    "    print(\"Regression Plot done: \"+ regression_plot_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B5rbaVCCbGbx",
    "outputId": "debd0c51-93fa-4ee4-bc50-f34499970894"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "837b_D-5LwYe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
